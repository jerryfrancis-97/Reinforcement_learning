{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup the display\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make DQN object\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self,height,width):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=height*width*3, out_features=24)\n",
    "        self.fc2 = nn.Linear(in_features=24, out_features=32)\n",
    "        self.out = nn.Linear(in_features=32, out_features=2)\n",
    "    def forward(self,x):\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "experience(state=2, action=1, next_state=4, reward=5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For replay memory, we use named tuple for storing info : maps field to values\n",
    "from collections import namedtuple\n",
    "Replay_exp = namedtuple('experience',('state','action','next_state', 'reward'))\n",
    "e = Replay_exp(2,1,4,5)\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replay memory object\n",
    "class ReplayMemory():\n",
    "    def __init__(self,size):\n",
    "        self.size = size\n",
    "        self.memory = []\n",
    "        self.no_exp = 0\n",
    "        \n",
    "    #To add exerience in memory\n",
    "    def push(self, Replay_exp):\n",
    "        if len(self.memory)<self.size:\n",
    "            self.memory.append(Replay_exp)\n",
    "        else:\n",
    "            #remove the most previous experience\n",
    "            self.memory[self.no_exp % self.size] = Replay_exp\n",
    "        self.no_exp+=1\n",
    "    #sample experiences\n",
    "    def sample(self,batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    def make_sample(self,batch_size):\n",
    "        return batch_size<=len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * \\\n",
    "        math.exp(-1. * current_step * self.decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create agent\n",
    "class Agent():\n",
    "    def __init__(self,strategy,no_actions,device):\n",
    "        self.strategy = strategy\n",
    "        self.no_actions = no_actions\n",
    "        self.current_step = 0\n",
    "        self.device = device\n",
    "    def action(self, state, policy_nw):\n",
    "        self.current_step+=1\n",
    "        rate = self.strategy.get_exploration_rate(self.current_step)\n",
    "        \n",
    "        if rate>random.random():\n",
    "            action =  random.randrange(self.no_actions)\n",
    "            #print('type1: ',type(torch.tensor([action]).to(self.device)))\n",
    "            return torch.tensor([action]).to(self.device)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                #print('type2: ',type(torch.tensor(policy_nw(state).argmax(dim=1).item())))\n",
    "                return torch.tensor([policy_nw(state).argmax(dim=1).item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleManager():\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.env = gym.make('CartPole-v0').unwrapped\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "\n",
    "    def num_actions_available(self):\n",
    "        return self.env.action_space.n\n",
    "\n",
    "    def take_action(self, action):  \n",
    "        _, reward, self.done, _ = self.env.step(action.item())\n",
    "        return torch.tensor([reward], device=self.device)\n",
    "\n",
    "    def just_starting(self):\n",
    "        return self.current_screen is None\n",
    "\n",
    "    def get_state(self):\n",
    "        if self.just_starting() or self.done:\n",
    "            self.current_screen = self.get_processed_screen()\n",
    "            black_screen = torch.zeros_like(self.current_screen)\n",
    "            return black_screen\n",
    "        else:\n",
    "            s1 = self.current_screen\n",
    "            s2 = self.get_processed_screen()\n",
    "            self.current_screen = s2\n",
    "            return s2 - s1\n",
    "\n",
    "    def get_screen_height(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[2]\n",
    "\n",
    "    def get_screen_width(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[3]\n",
    "\n",
    "    def get_processed_screen(self):\n",
    "        screen = self.render('rgb_array').transpose((2, 0, 1)) # PyTorch expects CHW\n",
    "        screen = self.crop_screen(screen)\n",
    "        return self.transform_screen_data(screen)\n",
    "\n",
    "    def crop_screen(self, screen):\n",
    "        screen_height = screen.shape[1]\n",
    "\n",
    "        # Strip off top and bottom\n",
    "        top = int(screen_height * 0.4)\n",
    "        bottom = int(screen_height * 0.8)\n",
    "        screen = screen[:, top:bottom, :]\n",
    "        return screen\n",
    "\n",
    "    def transform_screen_data(self, screen):       \n",
    "        # Convert to float, rescale, convert to tensor\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "\n",
    "        # Use torchvision package to compose image transforms\n",
    "        resize = T.Compose([\n",
    "            T.ToPILImage()\n",
    "            ,T.Resize((40,90))\n",
    "            ,T.ToTensor()\n",
    "        ])\n",
    "\n",
    "        return resize(screen).unsqueeze(0).to(self.device) # add a batch dimension (BCHW)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "em = CartPoleManager(device)\n",
    "em.reset()\n",
    "screen = em.render('rgb_array')\n",
    "\n",
    "#Starting state visualization\n",
    "screen = em.get_state()\n",
    "    \n",
    "plt.figure()\n",
    "plt.imshow(screen.squeeze(0).permute(1, 2, 0).cpu(), interpolation='none')\n",
    "plt.title('start_state')\n",
    "plt.show()\n",
    "em.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(values, moving_avg_period):\n",
    "    plt.figure(2)\n",
    "    plt.clf()        \n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(values)\n",
    "\n",
    "    moving_avg = get_moving_average(moving_avg_period, values)\n",
    "    plt.plot(moving_avg)    \n",
    "    plt.pause(0.001)\n",
    "    print(\"Episode\", len(values), \"\\n\", \\\n",
    "        moving_avg_period, \"episode moving avg:\", moving_avg[-1])\n",
    "    if is_ipython: display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moving_average(period, values):\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    if len(values) >= period:\n",
    "        moving_avg = values.unfold(dimension=0, size=period, step=1) \\\n",
    "            .mean(dim=1).flatten(start_dim=0)\n",
    "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
    "        return moving_avg.numpy()\n",
    "    else:\n",
    "        moving_avg = torch.zeros(len(values))\n",
    "        return moving_avg.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1],\n",
      "        [2, 2, 2, 2],\n",
      "        [3, 3, 3, 3]])\n",
      "(tensor([1, 2, 3]), tensor([1, 2, 3]), tensor([1, 2, 3]), tensor([1, 2, 3]))\n"
     ]
    }
   ],
   "source": [
    "def extract_tensors(experiences):\n",
    "    # Convert batch of Experiences to Experience of batches\n",
    "    batch = Replay_exp(*zip(*experiences))\n",
    "    #print('Batch action: ',batch.action,\"----------\")\n",
    "    t1 = torch.stack(batch.state, dim=0)\n",
    "    t2 = torch.stack(batch.action, dim=0)\n",
    "    t3 = torch.stack(batch.reward, dim=0)\n",
    "    t4 = torch.stack(batch.next_state, dim=0)\n",
    "    #print(t1,'||',t2,\"||\",t3,\"||\",t4)\n",
    "    return (t1,t2,t3,t4)\n",
    "\n",
    "e1 = Replay_exp(1,1,1,1)\n",
    "e2 = Replay_exp(2,2,2,2)\n",
    "e3 = Replay_exp(3,3,3,3)\n",
    "\n",
    "experiences = torch.tensor([e1,e2,e3])\n",
    "print(experiences)\n",
    "batch = extract_tensors(experiences)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper-parameters\n",
    "\n",
    "batch_size = 256\n",
    "gamma = 0.999\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.001\n",
    "target_update = 10\n",
    "memory_size = 100000\n",
    "lr = 0.001\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "em = CartPoleManager(device)\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "\n",
    "agent = Agent(strategy, em.num_actions_available(), device)\n",
    "memory = ReplayMemory(memory_size)\n",
    "\n",
    "policy_nw = DQN(em.get_screen_height(), em.get_screen_width()).to(device)\n",
    "target_nw = DQN(em.get_screen_height(), em.get_screen_width()).to(device)\n",
    "\n",
    "#copy weights into targets\n",
    "target_nw.load_state_dict(policy_nw.state_dict())\n",
    "#target n/w not in training mode\n",
    "target_nw.eval()\n",
    "optimizer = optim.Adam(params=policy_nw.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValues():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        print('states: ',policy_net(states).shape)\n",
    "        print('actions: ',actions.shape)\n",
    "#         print('actions: ',actions.unsqueeze(-1)[0])\n",
    "#         print('result: ',policy_net(states).gather(dim=1, index=actions))\n",
    "        return policy_net(states).gather(dim=1, index=actions) \n",
    "        \n",
    "\n",
    "    @staticmethod        \n",
    "    def get_next(target_net, next_states):                \n",
    "        final_state_locations = next_states.flatten(start_dim=1) \\\n",
    "            .max(dim=1)[0].eq(0).type(torch.bool)\n",
    "        non_final_state_locations = (final_state_locations == False)\n",
    "        non_final_states = next_states[non_final_state_locations]\n",
    "        batch_size = next_states.shape[0]\n",
    "        values = torch.zeros(batch_size).to(QValues.device)\n",
    "        values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings \n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states:  torch.Size([256, 2])\n",
      "actions:  torch.Size([256, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/venv1/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Using a target size (torch.Size([256, 1, 256])) that is different to the input size (torch.Size([256, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    }
   ],
   "source": [
    "episode_durations = []\n",
    "for episode in range(num_episodes):\n",
    "    em.reset()\n",
    "    state = em.get_state()\n",
    "    for timestep in count():\n",
    "        action = agent.action(state, policy_nw)\n",
    "        #print('tiemstep',timestep,'action',action)\n",
    "        reward = em.take_action(action)\n",
    "        next_state = em.get_state()\n",
    "        memory.push(Replay_exp(state, action, next_state, reward))\n",
    "        state = next_state\n",
    "        \n",
    "        if memory.make_sample(batch_size):\n",
    "            experiences = memory.sample(batch_size)\n",
    "            states, actions, rewards, next_states = extract_tensors(experiences)\n",
    "            current_qvalues = QValues.get_current(policy_nw, states, actions)\n",
    "            next_qvalues = QValues.get_next(target_nw, next_states)\n",
    "            target_qvalues = rewards + gamma*next_qvalues\n",
    "            \n",
    "            \n",
    "            loss = F.mse_loss(current_qvalues, target_qvalues.unsqueeze(1))\n",
    "            #to remove the gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if em.done:\n",
    "            episode_durations.append(timestep)\n",
    "            plot(episode_durations, 100)\n",
    "            break\n",
    "    if episode % target_update == 0:\n",
    "        target_nw.load_state_dict(policy_nw.state_dict())\n",
    "\n",
    "em.close()          \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n",
      "torch.Size([5, 1])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
      "        [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
      "        [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n",
      "        [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1],\n",
       "        [12],\n",
       "        [21],\n",
       "        [31],\n",
       "        [41]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Working of gather\n",
    "#dim=1 is referring to columns\n",
    "import torch\n",
    "import random\n",
    "t = torch.arange(50).reshape((5,10))\n",
    "a = torch.tensor([1,2,1,1,1]).reshape((5,1))\n",
    "print(t.shape)\n",
    "print(a.shape)\n",
    "print(t)\n",
    "print(a)\n",
    "torch.gather(t, 1, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
